{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6c512c",
   "metadata": {},
   "source": [
    "# Developing LLM Applications with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cce8c",
   "metadata": {},
   "source": [
    "# 1. Introduction to LangChain & Chatbot Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146d99f",
   "metadata": {},
   "source": [
    "## The LangChain ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20510d76",
   "metadata": {},
   "source": [
    "### Hugging Face models in LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Set your Hugging Face API token \n",
    "huggingfacehub_api_token = 'XXX'\n",
    "\n",
    "# Define the LLM\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\"\"\"\n",
    "! Minimal wear and tear on your shoes can be achieved with hard core 3 lbs. of Metal Mash.\n",
    "Invest in a new set of shoes or wear them into the ground. Or, take a fancy to replacing your old metal shoes was the new \n",
    "super-stylish trend this year. Our Metal Mash is the perfect way to replace that worn part in your shoes. Metal Mash is a \n",
    "long-lasting, general use, mild abrasive for general clean-up of metal parts. (Suitable... \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424a7ba",
   "metadata": {},
   "source": [
    "### OpenAI models in LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Set your API Key from OpenAI\n",
    "openai_api_key = \"<OPENAI_API_TOKEN>\" \n",
    "\n",
    "# Define the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a36fbe",
   "metadata": {},
   "source": [
    "## Prompting strategies for chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b533b",
   "metadata": {},
   "source": [
    "### Prompt templates and chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ed07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Hugging Face API token\n",
    "huggingfacehub_api_token = 'XXX'\n",
    "\n",
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.run(question))\n",
    "\n",
    "\"\"\"\n",
    "LangChain simplifies LLM application development by providing practical, customizable templates and features that \n",
    "enable users to easily create and maintain multilingual PDF files. Our templates for legal documents, such as contracts \n",
    "and agreements, are available in multiple languages with built-in options, saving time and effort for legal professionals.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ec1e5",
   "metadata": {},
   "source": [
    "### Chat prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key= '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question='How can I retain learning?')\n",
    "llm(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53804913",
   "metadata": {},
   "source": [
    "## Managing chat model memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238468d",
   "metadata": {},
   "source": [
    "### Integrating a chatbot message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ea595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key= '<OPENAI_API_TOKEN>'\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the conversation history and add the first AI message\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
    "\n",
    "# Add the user message to the history and call the model\n",
    "history.add_user_message(\"What is a list comprehension?\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)\n",
    "\n",
    "# Add another user message and call the model\n",
    "history.add_user_message(\"Describe the same in fewer words\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4d753",
   "metadata": {},
   "source": [
    "### Creating a memory buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a buffer memory\n",
    "memory = ConversationBufferMemory(size=4)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Write Python code to draw a scatter plot.\")\n",
    "buffer_chain.predict(input=\"Use the Seaborn library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4601c",
   "metadata": {},
   "source": [
    "### Implementing a summary memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a summary memory that uses an OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key))\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.predict(input=\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca844c0",
   "metadata": {},
   "source": [
    "# 2. Loading and Preparing External Data for Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5478ce",
   "metadata": {},
   "source": [
    "## Integrating document loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8c75d",
   "metadata": {},
   "source": [
    "### PDF document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for attention_is_all_you_need.pdf\n",
    "loader = PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "\n",
    "\"\"\"\n",
    "page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures\n",
    "in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle \n",
    "Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob \n",
    "Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. .......\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed735645",
   "metadata": {},
   "source": [
    "### CSV document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Create a document loader for fifa_countries_audience.csv\n",
    "loader = CSVLoader(file_path=\"fifa_countries_audience.csv\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "\n",
    "\"\"\"\n",
    "page_content='country: United States\\nconfederation: CONCACAF\\npopulation_share: 4.5\\ntv_audience_share: \n",
    "4.3\\ngdp_weighted_share: 11.3' metadata={'source': 'fifa_countries_audience.csv', 'row': 0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f90d7",
   "metadata": {},
   "source": [
    "### Third-party document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import HNLoader\n",
    "\n",
    "# Create a document loader for the top Hacker News stories\n",
    "loader = HNLoader(\"https://news.ycombinator.com\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "\n",
    "# Print the first document\n",
    "print(data[0])\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)\n",
    "\n",
    "\"\"\"\n",
    "page_content='Vala Programming Language (vala.dev)' metadata={'source': 'https://news.ycombinator.com', 'title': \n",
    "'Vala Programming Language (vala.dev)', 'link': 'https://vala.dev/', 'ranking': '1.'}\n",
    "\n",
    "{'source': 'https://news.ycombinator.com', 'title': 'Vala Programming Language (vala.dev)', 'link': 'https://vala.dev/', \n",
    "'ranking': '1.'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e30a1",
   "metadata": {},
   "source": [
    "## Splitting external data for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcbc588",
   "metadata": {},
   "source": [
    "### Splitting by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libary\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote) \n",
    "print(docs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "['One machine can do the work of fifty ordinary humans', 'No machine can do the work of one extraordinary human']\n",
    "['One machine can do the w', 'e work of fifty ordinary', 'ary humans. No machine c', 'e can do the work of one', \n",
    "'one extraordinary human.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e7ba4",
   "metadata": {},
   "source": [
    "### Recursively splitting by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13450379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libary\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=24,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)\n",
    "\n",
    "\"\"\"\n",
    "['Words are flowing out', 'out like endless rain', 'rain into a paper cup,', 'they slither while they', 'they pass,', \n",
    "'they slip away across', 'across the universe.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c042bdc",
   "metadata": {},
   "source": [
    "### Splitting HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HTML document into memory\n",
    "loader = UnstructuredHTMLLoader(\"white_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "# Define variables\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Split the HTML\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data) \n",
    "print(docs)\n",
    "\n",
    "\"\"\"\n",
    "[Document(page_content='To search this site, enter a search term\\n\\nSearch\\n\\nExecutive Order on the Safe, Secure, \n",
    "and Trustworthy Development and Use of Artificial Intelligence\\n\\nHome\\n\\nBriefing Room\\n\\nPresidential Actions\\n\\nBy \n",
    "the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby \n",
    "ordered as follows:\\n\\nSection 1', metadata={'source': 'white_house_executive_order_nov_2023.html'}), \n",
    "Document(page_content='.\\xa0 Purpose.\\xa0 Artificial intelligence (AI) holds extraordinary potential for both promise \n",
    "and peril....... \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58b6c7",
   "metadata": {},
   "source": [
    "## RAG storage and retrieval using vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f781d5",
   "metadata": {},
   "source": [
    "### Preparing the documents and vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Define an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the Chroma vector DB using the OpenAI embedding function; persist the database\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/',\n",
    "    embedding_function=embedding_model)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e4342",
   "metadata": {},
   "source": [
    "### Storing and retrieving documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285781fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"What is the primary architecture presented in the document?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc6b3d",
   "metadata": {},
   "source": [
    "### RAG with sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7404e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "loader = PyPDFLoader('attention_is_all_you_need.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the function for the question to be answered with\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the query on the documents\n",
    "results = qa({\"question\": \"What is the primary architecture presented in the document?\"}, return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d3127",
   "metadata": {},
   "source": [
    "# 3. LangChain Expression Language (LCEL), Chains, and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81ab45",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd1083",
   "metadata": {},
   "source": [
    "### LCEL for LLM chatbot chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your OpenAI API Key\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "prompt = ChatPromptTemplate.from_template(\"You are a skilled poet. Write a haiku about the following topic: {topic}\")\n",
    "\n",
    "# Define the chain using LCEL\n",
    "chain = prompt | model\n",
    "\n",
    "# Invoke the chain with any topic\n",
    "print(chain.invoke({\"topic\": \"Large Language Models\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4167e2a3",
   "metadata": {},
   "source": [
    "### LCEL for RAG workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your OpenAI API Key\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Create the retriever and model\n",
    "vectorstore = Chroma.from_texts([\"LangChain v0.1.0 was released on January 8, 2024.\"], embedding=OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
    "retriever = vectorstore.as_retriever()\n",
    "model = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "\n",
    "template = \"\"\"Answer the question based on the context:{context}. Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the chain and run it\n",
    "chain = (\n",
    "  {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "  | prompt\n",
    "  | model)\n",
    "\n",
    "chain.invoke(\"When was LangChain v0.1.0 released?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2292486",
   "metadata": {},
   "source": [
    "## Implementing functional LangChain chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe0f14",
   "metadata": {},
   "source": [
    "### Sequential chains with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "coding_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Write Python code to loop through the following list, printing each element: {list}\"\"\")\n",
    "validate_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Consider the following Python code: {answer} If it doesn't use a list comprehension, update it to use one. If it does use a list comprehension, return the original code without explanation:\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the sequential chain\n",
    "chain = ({\"answer\": coding_prompt | llm | StrOutputParser()}\n",
    "         | validate_prompt\n",
    "         | llm \n",
    "         | StrOutputParser() )\n",
    "\n",
    "# Invoke the chain with the user's question\n",
    "chain.invoke({\"list\": \"[3, 1, 4, 1]\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7fce57",
   "metadata": {},
   "source": [
    "### Passing values between chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Make ceo_response available for other chains\n",
    "ceo_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a CEO. Describe the most lucrative consumer product addressing the following consumer need in one sentence: {input}.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | {\"ceo_response\": RunnablePassthrough() | StrOutputParser()}\n",
    ")\n",
    "\n",
    "advisor_response = (\n",
    "    ChatPromptTemplate.from_template(\"You are a strategic adviser. Briefly map the outline and business plan for {ceo_response} in 3 key steps.\")\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "overall_response = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"CEO response:\\n{ceo_response}\\n\\nAdvisor response:\\n{advisor_response}\"),\n",
    "            (\"system\", \"Generate a final response including the CEO's response, the advisor response, and a summary of the business plan in one sentence.\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain to insert the outputs from the other chains into overall_response\n",
    "business_idea_chain = (\n",
    "    {\"ceo_response\": ceo_response, \"advisor_response\": advisor_response}\n",
    "    | overall_response\n",
    "    | ChatOpenAI(openai_api_key=openai_api_key)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(business_idea_chain.invoke({\"input\": \"Typing on mobile touchscreens is slow.\", \"ceo_response\": \"\", \"advisor_response\": \"\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a131cdd",
   "metadata": {},
   "source": [
    "## An introduction to LangChain agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627d293",
   "metadata": {},
   "source": [
    "### Zero-Shot ReAct agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define the tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "\n",
    "# Define the agent\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "agent.run(\"What is 10 multiplied by 50?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463f7ff",
   "metadata": {},
   "source": [
    "# 4. Tools, Troubleshooting, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a9f8e",
   "metadata": {},
   "source": [
    "## Utilizing tools in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b5044",
   "metadata": {},
   "source": [
    "### Creating custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef67add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Define the calculate_ltv tool function\n",
    "@tool\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"LTV Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "# Define the tools list\n",
    "tools = [Tool(name=\"LTVReport\",\n",
    "              func=calculate_ltv,\n",
    "              description=\"Use this for calculating historical LTV.\")]\n",
    "\n",
    "# Initialize the appropriate agent type\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"Run a financial report that calculates historical LTV for Hooli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd968d",
   "metadata": {},
   "source": [
    "### Scaling custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "def calculate_wellness_score(sleep_hours, exercise_minutes, healthy_meals, stress_level):\n",
    "    \"\"\"Calculate a Wellness Score based on sleep, exercise, nutrition, and stress management.\"\"\"\n",
    "    max_score_per_category = 25\n",
    "\n",
    "    sleep_score = min(sleep_hours / 8 * max_score_per_category, max_score_per_category)\n",
    "    exercise_score = min(exercise_minutes / 30 * max_score_per_category, max_score_per_category)\n",
    "    nutrition_score = min(healthy_meals / 3 * max_score_per_category, max_score_per_category)\n",
    "    stress_score = max_score_per_category - min(stress_level / 10 * max_score_per_category, max_score_per_category)\n",
    "\n",
    "    total_score = sleep_score + exercise_score + nutrition_score + stress_score\n",
    "    return total_score\n",
    "\n",
    "# Create a structured tool from calculate_wellness_score()\n",
    "tools = [StructuredTool.from_function(calculate_wellness_score)]\n",
    "\n",
    "# Initialize the appropriate agent type and tool set\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "wellness_tool = tools[0]\n",
    "result = wellness_tool.func(sleep_hours=8, exercise_minutes=14, healthy_meals=10, stress_level=20)\n",
    "print(result)\n",
    "\n",
    "#  61.666666666666664"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aa57b",
   "metadata": {},
   "source": [
    "### Formatting tools as OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98627e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LTVDescription class to manually add a function description\n",
    "class LTVDescription(BaseModel):\n",
    "    query: str = Field(description='Calculate an extremely simple historical LTV')\n",
    "\n",
    "# Format the calculate_ltv tool function so it can be used by OpenAI models\n",
    "@tool(args_schema=LTVDescription)\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company to pontificate with.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"Pontification Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "print(format_tool_to_openai_function(calculate_ltv))\n",
    "\n",
    "\"\"\"\n",
    "{'name': 'calculate_ltv', 'description': 'calculate_ltv(company_name: str) -> str - Generate the LTV for a company \n",
    "to pontificate with.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'Calculate an extremely \n",
    "simple historical LTV', 'type': 'string'}}, 'required': ['query']}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafee43",
   "metadata": {},
   "source": [
    "## Troubleshooting methods for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134d79a",
   "metadata": {},
   "source": [
    "### Callbacks for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Complete the CallingItIn class to return the prompt, model_name, and temperature\n",
    "class CallingItIn(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, invocation_params, **kwargs):\n",
    "        print(prompts) \n",
    "        print(invocation_params[\"model_name\"])  \n",
    "        print(invocation_params[\"temperature\"]) \n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", streaming=True, openai_api_key=openai_api_key)\n",
    "prompt_template = \"What do {animal} like to eat?\"\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Call the model with the parameters needed by the prompt\n",
    "output = chain.run({\"animal\": \"wombats\"}, callbacks=[CallingItIn()])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624662c",
   "metadata": {},
   "source": [
    "### Real-time performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Complete the PerformanceMonitoringCallback class to return the token and time\n",
    "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    print(f\"Token: {repr(token)} generated at time: {time.time()}\")\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key, temperature=0, streaming=True)\n",
    "prompt_template = \"Describe the process of photosynthesis.\"\n",
    "chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Call the chain with the callback\n",
    "output = chain.run({}, callbacks=[PerformanceMonitoringCallback()])\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66f0fd",
   "metadata": {},
   "source": [
    "## Evaluating model output in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c2363",
   "metadata": {},
   "source": [
    "### Built-in evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Load evaluator, assign it to criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"42\",\n",
    "    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n",
    ")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e3e08",
   "metadata": {},
   "source": [
    "### Custom evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d301ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "# Add a scalability criterion to custom_criteria\n",
    "custom_criteria = {\n",
    "    \"market_potential\": \"Does the suggestion effectively assess the market potential of the startup?\",\n",
    "    \"innovation\": \"Does the suggestion highlight the startup's innovation and uniqueness in its sector?\",\n",
    "    \"risk_assessment\": \"Does the suggestion provide a thorough analysis of potential risks and mitigation strategies?\",\n",
    "    \"scalability\": \"Does the suggestion address the startup's scalability and growth potential?\"\n",
    "}\n",
    "\n",
    "# Criteria an evaluator from custom_criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria, llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Should I invest in a startup focused on flying cars? The CEO won't take no for an answer from anyone.\",\n",
    "    prediction=\"No, that is ridiculous.\")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4b8be",
   "metadata": {},
   "source": [
    "### Evaluation chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = '<OPENAI_API_TOKEN>'\n",
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding)\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever(), input_key=\"question\")\n",
    "\n",
    "# Generate the model responses using the RetrievalQA chain and question_set\n",
    "predictions = qa.apply(question_set)\n",
    "\n",
    "# Define the evaluation chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Evaluate the ground truth against the answers that are returned\n",
    "results = eval_chain.evaluate(question_set,\n",
    "                              predictions,\n",
    "                              question_key=\"question\",\n",
    "                              prediction_key=\"result\",\n",
    "                              answer_key='answer')\n",
    "\n",
    "for i, q in enumerate(question_set):\n",
    "    print(f\"Question {i+1}: {q['question']}\")\n",
    "    print(f\"Expected Answer: {q['answer']}\")\n",
    "    print(f\"Model Prediction: {predictions[i]['result']}\\n\")\n",
    "    \n",
    "print(results)\n",
    "\n",
    "\"\"\" \n",
    "[{'question': 'What is the primary architecture presented in the document?', 'answer': 'The Transformer.'}, \n",
    "{'question': 'According to the document, is the Transformer faster or slower than architectures based on recurrent \n",
    "or convolutional layers?', 'answer': 'The Transformer is faster.'}, {'question': 'Who is the primary author of the document?',\n",
    "'answer': 'Ashish Vaswani.'}]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
