{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d726c7",
   "metadata": {},
   "source": [
    "# Working with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f73b96",
   "metadata": {},
   "source": [
    "# 1. Getting Started with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a6b77",
   "metadata": {},
   "source": [
    "## Introduction to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f006f6",
   "metadata": {},
   "source": [
    "## Transformers and the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896335a",
   "metadata": {},
   "source": [
    "### Searching the Hub with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n",
    "from huggingface_hub import HfApi\n",
    "list(api.list_models())\n",
    "\n",
    "# Create the instance of the API\n",
    "api = HfApi()\n",
    "\n",
    "# Return the filtered list from the Hub\n",
    "models = api.list_models(\n",
    "    filter=ModelFilter(task=\"text-classification\"),\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "  \tlimit=1\n",
    ")\n",
    "\n",
    "# Store as a list\n",
    "modelList = list(models)\n",
    "\n",
    "print(modelList[0].modelId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0be5d6",
   "metadata": {},
   "source": [
    "### Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff217fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Instantiate the AutoModel class\n",
    "model = AutoModel.from_pretrained(modelId)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory=f\"models/{modelId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939bc13",
   "metadata": {},
   "source": [
    "## Working with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets\n",
    "\n",
    "# Load the module\n",
    "from datasets import load_dataset_builder\n",
    "\n",
    "# Create the dataset builder\n",
    "reviews_builder = load_dataset_builder(\"derenrich/wikidata-en-descriptions-small\")\n",
    "\n",
    "# Print the features\n",
    "print(reviews_builder.info.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dce204",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32109c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train portion of the dataset\n",
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", language=\"20231101.en\", split=\"train\")\n",
    "\n",
    "print(f\"The length of the dataset is {len(wikipedia)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccc63e",
   "metadata": {},
   "source": [
    "### Manipulating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the documents\n",
    "filtered = wikipedia.filter(lambda row: 'football' in row[\"text\"])\n",
    "\n",
    "# Create a sample dataset\n",
    "example = filtered.select(range(1))\n",
    "\n",
    "print(example[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41aa60",
   "metadata": {},
   "source": [
    "# 2. Building Pipelines with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d5a24f",
   "metadata": {},
   "source": [
    "## Pipelines with Hugging Face\n",
    "\n",
    "### Geting started with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create the task pipeline\n",
    "task_pipeline = pipeline(task=\"sentiment-analysis\")\n",
    "\n",
    "# Create the model pipeline\n",
    "model_pipeline = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Predict the sentiment\n",
    "task_output = task_pipeline(input)\n",
    "model_output = model_pipeline(input)\n",
    "\n",
    "print(f\"Sentiment from task_pipeline: {task_output[0]['label']}; Sentiment from model_pipeline: {model_output[0]['label']}\")\n",
    "\n",
    "### <script.py> output: Sentiment from task_pipeline: POSITIVE; Sentiment from model_pipeline: POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242c475",
   "metadata": {},
   "source": [
    "### Using AutoClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d298e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Create the pipeline\n",
    "sentimentAnalysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Predict the sentiment\n",
    "output = sentimentAnalysis(input)\n",
    "\n",
    "print(f\"Sentiment using AutoClasses: {output[0]['label']}\")\n",
    "\n",
    "### <script.py> output: Sentiment using AutoClasses: POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78e1d3",
   "metadata": {},
   "source": [
    "### Comparing models with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "distil_pipeline = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Predict the sentiment\n",
    "distil_output = distil_pipeline(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "distil_pipeline = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Predict the sentiment\n",
    "distil_output = distil_pipeline(input)\n",
    "\n",
    "# Create the second pipeline and predict the sentiment\n",
    "bert_pipeline = pipeline(task=\"sentiment-analysis\", model=\"kwang123/bert-sentiment-analysis\")\n",
    "bert_output = bert_pipeline(input)\n",
    "\n",
    "print(f\"Bert Output: {bert_output[0]['label']}\")\n",
    "print(f\"Distil Output: {distil_output[0]['label']}\")\n",
    "\n",
    "# <script.py> output: Bert Output: Extremely Positive     Distil Output: POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e0d9e",
   "metadata": {},
   "source": [
    "## NLP and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c86c59",
   "metadata": {},
   "source": [
    "### Normalizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e22f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Normalize the input string\n",
    "output = tokenizer.backend_tokenizer.normalizer.normalize_str(\"HOWDY, how aré yoü?\")\n",
    "\n",
    "print(output)\n",
    "\n",
    "# <script.py> output: howdy, how are you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fffd5",
   "metadata": {},
   "source": [
    "### Comparing tokenizer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the gpt tokenizer\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize the input\n",
    "gpt_tokens = gpt_tokenizer.tokenize(text=input)\n",
    "\n",
    "# Repeat for distilbert\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "distil_tokens = distil_tokenizer.tokenize(text=input)\n",
    "\n",
    "# Compare the output\n",
    "print(f\"GPT tokenizer: {gpt_tokens}\")\n",
    "print(f\"DistilBERT tokenizer: {distil_tokens}\")\n",
    "\n",
    "# <script.py> output:\n",
    "# GPT tokenizer: ['P', 'ine', 'apple', 'Ġon', 'Ġpizza', 'Ġis', 'Ġpretty', 'Ġgood', ',', 'ĠI', 'Ġguess', '.']\n",
    "# DistilBERT tokenizer: ['pine', '##apple', 'on', 'pizza', 'is', 'pretty', 'good', ',', 'i', 'guess', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc5883",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956e3cd",
   "metadata": {},
   "source": [
    "### Grammatical correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bdb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "classifier = pipeline(\n",
    "  task=\"text-classification\", \n",
    "  model=\"abdulmatinomotoso/English_Grammar_Checker\"\n",
    ")\n",
    "\n",
    "# Predict classification\n",
    "output = classifier(\"I will walk dog\")\n",
    "\n",
    "print(output)\n",
    "\n",
    "# <script.py> output: [{'label': 'LABEL_0', 'score': 0.9956323504447937}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd78395",
   "metadata": {},
   "source": [
    "### Question Natural Language Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "classifier = pipeline(task=\"text-classification\", model=\"cross-encoder/qnli-electra-base\")\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(\"Where is the capital of France?, Brittany is known for their kouign-amann.\")\n",
    "\n",
    "print(output)\n",
    "\n",
    "# <script.py> output: [{'label': 'LABEL_0', 'score': 0.005238980986177921}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c7e85",
   "metadata": {},
   "source": [
    "### Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98569bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the zero-shot classifier\n",
    "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Create the list\n",
    "candidate_labels = [\"politics\", \"science\", \"sports\"]\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(text, candidate_labels)\n",
    "\n",
    "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]}\")\n",
    "\n",
    "# <script.py> output: Top Label: science with score: 0.9030616879463196"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc999b",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b3b5e",
   "metadata": {},
   "source": [
    "### Summarizing long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the summarization pipeline\n",
    "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
    "\n",
    "# Summarize the text\n",
    "summary_text = summarizer(original_text)\n",
    "\n",
    "# Compare the length\n",
    "print(f\"Original text length: {len(original_text)}\")\n",
    "print(f\"Summary length: {len(summary_text[0]['summary_text'])}\")\n",
    "\n",
    "# <script.py> output:\n",
    "# Original text length: 829\n",
    "# Summary length: 473"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12049296",
   "metadata": {},
   "source": [
    "### Using min_length and max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ed34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a short summarizer\n",
    "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
    "\n",
    "# Summarize the input text\n",
    "short_summary_text = short_summarizer(original_text)\n",
    "\n",
    "# Print the short summary\n",
    "print(short_summary_text[0][\"summary_text\"])\n",
    "\n",
    "# <script.py> output:\n",
    "# Greece has many islands, with estimates ranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a short summarizer\n",
    "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=1, max_length=10)\n",
    "\n",
    "# Summarize the input text\n",
    "short_summary_text = short_summarizer(original_text)\n",
    "\n",
    "# Print the short summary\n",
    "print(short_summary_text[0][\"summary_text\"])\n",
    "\n",
    "# Repeat for a long summarizer\n",
    "long_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_length=50, max_length=150)\n",
    "\n",
    "long_summary_text = long_summarizer(original_text)\n",
    "\n",
    "# Print the long summary\n",
    "print(long_summary_text[0][\"summary_text\"])\n",
    "\n",
    "\"\"\" Greece has many islands, with estimates ranging from somewhere around 1,200 to 6,000 depending on the minimum size \n",
    "to take into account. The number of inhabited islands is variously cited as between 166 and 227. The Greek islands are \n",
    "traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, \n",
    "a large but dense collection occupying the central part of the Aegean Sea; the North Aegesan islands, an loose group \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46374400",
   "metadata": {},
   "source": [
    "### Summarizing several inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff611e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list\n",
    "text_to_summarize = [w[\"text\"] for w in wiki]\n",
    "\n",
    "# Create the pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"cnicu/t5-small-booksum\",min_length=20, max_length=50)\n",
    "\n",
    "# Summarize each item in the list\n",
    "summaries = summarizer(text_to_summarize[:3], truncation=True)\n",
    "\n",
    "# Create for-loop to print each summary\n",
    "for i in range(0,3):\n",
    "  print(f\"Summary {i+1}: {summaries[i]['summary_text']}\")\n",
    "\n",
    "\"\"\" Summary 1: The Serapeum of Saqqara was the ancient Egyptian burial place for sacred bulls of the Apis cult at Memphis. \n",
    "It was believed that the bulls were incarnations of the god Ptah,\n",
    "    Summary 2: Sauda is a town in Rogaland county, Norway. The town, which is also the administrative centre of the municipality,\n",
    "is located in a river valley at the northern end of the town centre. A large part of the\n",
    "    Summary 3: Luis Miguel Aparecido Alves (born May 25, 1985), known as Gugu, is a Brazilian football player currently playing \n",
    "for Iraklis Psachna F.C. External links 1985 births Living people\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c6894e",
   "metadata": {},
   "source": [
    "# 3. Building Pipelines for Image and Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5f434",
   "metadata": {},
   "source": [
    "## Processing and classifying images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d3663",
   "metadata": {},
   "source": [
    "### Processing image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the numpy array\n",
    "image_array = np.array(original_image)\n",
    "\n",
    "# Crop the center of the image\n",
    "cropped_image = image_transforms.center_crop(image=image_array, size=(200, 200))\n",
    "\n",
    "imgplot = plt.imshow(cropped_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f0df9",
   "metadata": {},
   "source": [
    "### Creating an image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ca7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "image_classifier = pipeline(task=\"image-classification\", \n",
    "            model=\"abhishek/autotrain_fashion_mnist_vit_base\")\n",
    "\n",
    "# Predict the class of the image\n",
    "results = image_classifier(cropped_image)\n",
    "\n",
    "# Print the results\n",
    "print(results[0][\"label\"])\n",
    "\n",
    "# Create the pipeline\n",
    "image_classifier = pipeline(task=\"image-classification\", \n",
    "            model=\"abhishek/autotrain_fashion_mnist_vit_base\")\n",
    "\n",
    "# Predict the class of the image\n",
    "results = image_classifier(cropped_image)\n",
    "\n",
    "# <script.py> output:  Pullover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174cdc1",
   "metadata": {},
   "source": [
    "## Question answering and multi-modal tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e64566",
   "metadata": {},
   "source": [
    "### Document question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "dqa = pipeline(task=\"document-question-answering\", model=\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "# Set the image and question\n",
    "image = \"document.png\"\n",
    "question = \"Which meeting is this document about?\"\n",
    "\n",
    "# Get the answer\n",
    "results = dqa(image=image, question=question)\n",
    "\n",
    "print(results)\n",
    "\n",
    "# <script.py> output: [{'score': 0.7789, 'answer': 'takeda global risk management forum', 'start': 2, 'end': 7}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13421586",
   "metadata": {},
   "source": [
    "### Visual question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7603314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "vqa = pipeline(task=\"visual-question-answering\", model=\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Use image and question in vqa\n",
    "results = vqa(image=image, question=question)\n",
    "\n",
    "print(results)\n",
    "\n",
    "\"\"\" <script.py> output:  \n",
    "[{'score': 0.9795706272125244, 'answer': 'hat'}, {'score': 0.5232054591178894, 'answer': 'beanie'},\n",
    "{'score': 0.24782036244869232, 'answer': 'cap'}, {'score': 0.1803695112466812, 'answer': 'sweater'}, \n",
    "{'score': 0.021539464592933655, 'answer': 'hoodie'}] \"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6528ccc",
   "metadata": {},
   "source": [
    "## Audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e09048",
   "metadata": {},
   "source": [
    "### Resampling audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1048ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the old sampling rate\n",
    "old_sampling_rate = audio_file[1][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "# Resample the audio files\n",
    "audio_file = audio_file.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# Compare the old and new sampling rates\n",
    "print(\"Old sampling rate:\", old_sampling_rate)\n",
    "print(\"New sampling rate:\", audio_file[1][\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "# <script.py> output:\n",
    "\n",
    "# Old sampling rate: 48000\n",
    "# New sampling rate: 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5fe5e4",
   "metadata": {},
   "source": [
    "### Filtering out audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of durations\n",
    "old_durations_list = []\n",
    "\n",
    "# Loop over the dataset\n",
    "for row in dataset[\"path\"]:\n",
    "    old_durations_list.append(librosa.get_duration(path=row))\n",
    "\n",
    "# Create a new column\n",
    "dataset = dataset.add_column(\"duration\", old_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1029ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of durations\n",
    "old_durations_list = []\n",
    "\n",
    "# Loop over the dataset\n",
    "for row in dataset[\"path\"]:\n",
    "    old_durations_list.append(librosa.get_duration(path=row))\n",
    "\n",
    "# Create a new column\n",
    "dataset = dataset.add_column(\"duration\", old_durations_list)\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_dataset = dataset.filter(lambda d: d < 6.0, input_columns=[\"duration\"], keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of durations\n",
    "old_durations_list = []\n",
    "\n",
    "# Loop over the dataset\n",
    "for row in dataset[\"path\"]:\n",
    "    old_durations_list.append(librosa.get_duration(path=row))\n",
    "\n",
    "# Create a new column\n",
    "dataset = dataset.add_column(\"duration\", old_durations_list)\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_dataset = dataset.filter(lambda d: d < 6.0, input_columns=[\"duration\"], keep_in_memory=True)\n",
    "\n",
    "# Save new durations\n",
    "new_durations_list = filtered_dataset[\"duration\"]\n",
    "\n",
    "print(\"Old duration:\", np.mean(old_durations_list)) \n",
    "print(\"New duration:\", np.mean(new_durations_list))\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "    Old duration: 4.8\n",
    "    New duration: 3.3333333333333335\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406602b0",
   "metadata": {},
   "source": [
    "### Classifying audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b16d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "classifier = pipeline(task=\"audio-classification\", model=\"facebook/mms-lid-126\")\n",
    "\n",
    "# Extract the sample\n",
    "audio = dataset[1][\"audio\"][\"array\"]\n",
    "sentence = dataset[1][\"sentence\"]\n",
    "\n",
    "# Predict the language\n",
    "prediction = classifier(audio)\n",
    "\n",
    "print(f\"Predicted language is '{prediction[0]['label'].upper()}' for the sentence '{sentence}'\")\n",
    "\n",
    "# <script.py> output:\n",
    "# Predicted language is 'DEU' for the sentence 'Deswegen ballert es mehr.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5859c",
   "metadata": {},
   "source": [
    "## Automatic speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1388f9",
   "metadata": {},
   "source": [
    "### Instantiating an ASR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ASR pipeline using Meta's wav2vec model\n",
    "meta_asr = pipeline(task=\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Predict the text from the example audio\n",
    "meta_pred = meta_asr(example[\"audio\"][\"array\"])[\"text\"].lower()\n",
    "\n",
    "# Repeat for OpenAI's Whisper model\n",
    "open_asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny\")\n",
    "open_pred = open_asr(example[\"audio\"][\"array\"])[\"text\"].lower()\n",
    "\n",
    "# Print the prediction from both models\n",
    "print(\"META:\", meta_pred)\n",
    "print(\"OPENAI:\", open_pred)\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "    META: it is a charity school whose feeds are calculated on a men test\n",
    "    OPENAI:  it is a charity school whose fees are calculated on a means test.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279e13c",
   "metadata": {},
   "source": [
    "### Word error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ab0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word error rate metric\n",
    "wer = load(\"wer\")\n",
    "\n",
    "# Save the true sentence of the example\n",
    "true_sentence = example[\"sentence\"].lower()\n",
    "\n",
    "# Compute the wer for each model prediction\n",
    "meta_wer = wer.compute(predictions=[meta_pred], references=[true_sentence])\n",
    "open_wer = wer.compute(predictions=[open_pred], references=[true_sentence])\n",
    "\n",
    "print(f\"The WER for the Meta model is {meta_wer} and for the OpenAI model is {open_wer}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "    The WER for the Meta model is 0.23076923076923078 and for the OpenAI model is 0.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87719d",
   "metadata": {},
   "source": [
    "### Iterating over a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data function\n",
    "def data(n=3):\n",
    "    for i in range(n):\n",
    "        yield english[i][\"audio\"][\"array\"], english[i][\"sentence\"].lower()\n",
    "        \n",
    "# Predict the text for the audio file with both models\n",
    "output = []\n",
    "for audio, sentence in data():\n",
    "    meta_pred = meta_asr(audio)[\"text\"].lower()\n",
    "    open_pred = open_asr(audio)[\"text\"].lower()\n",
    "    # Append to output list\n",
    "    output.append({\"sentence\": sentence, \"metaPred\": meta_pred, \"openPred\": open_pred})\n",
    "\n",
    "output_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89acbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data function\n",
    "def data(n=3):\n",
    "    for i in range(n):\n",
    "        yield english[i][\"audio\"][\"array\"], english[i][\"sentence\"].lower()\n",
    "        \n",
    "# Predict the text for the audio file with both models\n",
    "output = []\n",
    "for audio, sentence in data():\n",
    "    meta_pred = meta_asr(audio)[\"text\"].lower()\n",
    "    open_pred = open_asr(audio)[\"text\"].lower()\n",
    "    # Append to output list\n",
    "    output.append({\"sentence\": sentence, \"metaPred\": meta_pred, \"openPred\": open_pred})\n",
    "\n",
    "output_df = pd.DataFrame(output)\n",
    "\n",
    "# Compute the WER for both models\n",
    "metaWER = wer.compute(predictions=output_df[\"metaPred\"], references=output_df[\"sentence\"])\n",
    "openWER = wer.compute(predictions=output_df[\"openPred\"], references=output_df[\"sentence\"])\n",
    "\n",
    "# Print the WER\n",
    "print(f\"The WER for the meta model is {metaWER} and for the open model is {openWER}\")\n",
    "\n",
    "# <script.py> output:\n",
    "# The WER for the meta model is 0.6097560975609756 and for the open model is 0.24390243902439024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef9ae7",
   "metadata": {},
   "source": [
    "# 4. Fine-tuning and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6cd54",
   "metadata": {},
   "source": [
    "## Fine-tuning a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270217bc",
   "metadata": {},
   "source": [
    "### Preparing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee63f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use tokenizer on text\n",
    "dataset = dataset.map(lambda row: tokenizer(row[\"text\"], padding=True, max_length=512, truncation=True), keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d6ee2",
   "metadata": {},
   "source": [
    "### Building the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training arguments\n",
    "training_args = TrainingArguments(output_dir=\"./results\")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=training_data, \n",
    "    eval_dataset=testing_data\n",
    ")\n",
    "\n",
    "# Start the trainer\n",
    "trainer.train()\n",
    "\n",
    "# Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e1c8c",
   "metadata": {},
   "source": [
    "### Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094de240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"./fine_tuned_model\")\n",
    "\n",
    "# Classify the text\n",
    "results = classifier(text=text_example)\n",
    "\n",
    "print(results)\n",
    "\n",
    "# [{'label': 'POSITIVE', 'score': 0.9999}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88b2f6",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f1d53",
   "metadata": {},
   "source": [
    "### Generating text from a text prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99426483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Get the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Get the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Wear sunglasses when its sunny because\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Get the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Wear sunglasses when its sunny because\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the text output\n",
    "output = model.generate(input_ids, num_return_sequences=1)\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(output[0])\n",
    "\n",
    "print(generated_text)\n",
    "\n",
    "\"\"\"\n",
    "    Wear sunglasses when its sunny because it's a hot day.\n",
    "    \n",
    "    The best way to get\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7f383",
   "metadata": {},
   "source": [
    "### Generating a caption for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae47dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "# Process the image\n",
    "pixels = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Generate the ids\n",
    "output = model.generate(pixel_values=pixels)\n",
    "\n",
    "# Decode the output\n",
    "caption = processor.batch_decode(output)\n",
    "\n",
    "print(caption[0])\n",
    "\n",
    "# [CLS] a woman wearing a black sweater and grey sweatpants. [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480cf7d",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f10da",
   "metadata": {},
   "source": [
    "### Generate embeddings for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first embedding model\n",
    "embedder1 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed the sentence\n",
    "embedding1 = embedder1.encode([sentence])\n",
    "\n",
    "# Create and use second embedding model\n",
    "embedder2 = SentenceTransformer(\"sentence-transformers/paraphrase-albert-small-v2\")\n",
    "embedding2 = embedder2.encode([sentence])\n",
    " \n",
    "# Compare the shapes\n",
    "print(embedding1.shape == embedding2.shape)\n",
    "\n",
    "# Print embedding1\n",
    "print(embedding1)\n",
    "\n",
    "\"\"\"\n",
    " False\n",
    "    [[-9.97783169e-02  2.52459422e-02 -3.83034647e-02 -3.18860300e-02\n",
    "      -2.63425820e-02  2.63856594e-02  1.59228127e-02 -4.64465283e-03\n",
    "       2.83517758e-03 -3.23289521e-02  6.51238114e-02  1.30132377e-01\n",
    "       9.38770995e-02 -1.41172754e-02  5.23919286e-03  9.25582573e-02\n",
    "      -1.23580517e-02 -4.03374620e-02 -7.99983889e-02 -6.15836121e-02\n",
    "       3.31274867e-02 -5.38319498e-02 -3.77525352e-02  2.82219537e-02\n",
    "       3.79624367e-02  3.03728953e-02  3.27130929e-02  4.29569818e-02\n",
    "       5.17940819e-02  5.30882627e-02 -6.03116266e-02  2.60443687e-02\n",
    "       3.02512143e-02  7.42835924e-02 -6.47228435e-02  5.04810400e-02\n",
    "      -1.77761260e-02  7.10309371e-02 -1.43843796e-03 -1.53305486e-03\n",
    "      -1.32317245e-01 -2.39030961e-02  2.89693419e-02  4.83148843e-02\n",
    "       2.22859662e-02  4.36660927e-03 -3.29253897e-02 -3.66645530e-02\n",
    "      -9.52791190e-04 -3.06845773e-02 -5.96390963e-02 -3.28561775e-02\n",
    "      -3.68510298e-02  1.21275438e-02  6.58628717e-02  5.20405285e-02\n",
    "       5.09343781e-02  4.50430065e-03  1.59408674e-02  2.60929926e-03\n",
    "       3.09519451e-02  2.70236451e-02 -4.74882945e-02  4.79927212e-02\n",
    "       4.69670035e-02  1.36781754e-02 -5.51799648e-02  5.68730496e-02\n",
    "      -6.44658580e-02  5.22145629e-02 -5.31799383e-02  1.09241001e-01\n",
    "      -3.63433966e-03  8.24928954e-02 -2.86205094e-02  7.33143762e-02\n",
    "      -1.69356037e-02  5.79553805e-02 -5.36082871e-02 -6.04650266e-02\n",
    "      -7.38946572e-02 -1.75970774e-02  1.68791804e-02  5.80657125e-02\n",
    "       3.73408720e-02 -3.50533053e-02  7.05841109e-02  1.13283354e-03\n",
    "       6.18120730e-02  1.58435311e-02  1.09596578e-02 -4.22663316e-02\n",
    "       4.71505821e-02  2.85447277e-02 -2.13031620e-02  4.17851917e-02\n",
    "       8.52201320e-03 -7.16882721e-02 -8.60826462e-04  1.62748601e-02\n",
    "       2.61676311e-02  6.76399767e-02  1.81852039e-02  9.94604081e-03\n",
    "       3.35503519e-02 -3.20622697e-02 -4.24551442e-02  2.48599797e-02\n",
    "       3.91133275e-04 -6.94354475e-02 -1.30046555e-03  5.69762699e-02\n",
    "      -1.68611649e-02 -4.50349152e-02 -1.57800727e-02 -3.37041132e-02\n",
    "       1.21849012e-02  1.07976552e-02  8.05590078e-02 -5.08098640e-02\n",
    "       9.08086747e-02  5.18481582e-02 -3.30031812e-02  6.11459576e-02\n",
    "       3.17238481e-03 -8.13124925e-02 -6.41077235e-02 -5.65072220e-33\n",
    "       5.49403429e-02 -4.77581918e-02 -5.90103120e-02  6.94325045e-02\n",
    "      -4.50356901e-02  3.03159505e-02 -1.01291820e-01  3.36031988e-02\n",
    "      -5.72547987e-02 -4.23307642e-02  3.79419141e-02 -1.04357138e-01\n",
    "       5.42505607e-02  4.75572832e-02 -5.57749756e-02 -2.26175506e-02\n",
    "       2.16718987e-02 -4.60104868e-02 -4.85511571e-02 -3.09738424e-02\n",
    "       2.51442622e-02 -4.17656377e-02 -5.00752293e-02  7.94275105e-02\n",
    "       2.81374343e-02  6.27281144e-02  2.36685146e-02 -5.19433506e-02\n",
    "      -9.22212098e-03  2.62258593e-02 -3.77551988e-02 -3.53067890e-02\n",
    "      -7.92817920e-02 -3.10328919e-02  2.87927911e-02  8.80100429e-02\n",
    "       3.60820740e-02 -4.85621840e-02  5.01015335e-02 -1.73070673e-02\n",
    "      -2.21038684e-02  4.00975794e-02 -3.99311185e-02 -7.84583949e-03\n",
    "       3.78126353e-02 -2.85196006e-02 -1.76647175e-02  1.87056698e-02\n",
    "       8.93504079e-03  1.75491534e-02 -1.15821296e-02  4.05076779e-02\n",
    "       8.65147561e-02 -3.49141611e-03  8.90456419e-03 -7.44589046e-02\n",
    "      -6.05441048e-04 -8.67986903e-02 -2.64925789e-02  3.04380972e-02\n",
    "      -4.57907282e-02  6.03588810e-03  3.32466629e-03 -3.11117992e-02\n",
    "       7.98699632e-03  8.80055130e-03 -1.57751311e-02  5.53784519e-02\n",
    "       1.39530167e-01 -8.32842961e-02 -5.11823185e-02  6.75496981e-02\n",
    "      -2.05617603e-02  3.17197293e-02 -5.99950403e-02 -7.21503934e-03\n",
    "      -7.75092095e-02  1.94495786e-02 -1.57112237e-02 -1.00682855e-01\n",
    "       5.32578863e-02  3.68943587e-02 -1.50739886e-02  3.64669710e-02\n",
    "      -1.66182453e-03 -1.37150940e-02  6.17486611e-02 -3.34757157e-02\n",
    "      -1.42693585e-02 -2.81503820e-03 -2.68557435e-03 -1.34696867e-02\n",
    "       8.08540210e-02 -1.67186167e-02 -4.53580171e-02  3.81941771e-33\n",
    "       3.51107158e-02 -7.32830586e-03  1.55316852e-03 -7.34632388e-02\n",
    "      -8.12715441e-02  3.36477719e-02  4.78136800e-02 -6.79817945e-02\n",
    "       4.49076332e-02  3.69781815e-02 -1.04608918e-02 -2.66188495e-02\n",
    "      -7.78917745e-02 -5.20985723e-02  4.12008911e-02  6.24233633e-02\n",
    "      -1.02880567e-01 -8.10468849e-03 -6.27486128e-03 -2.78272890e-02\n",
    "      -7.65276551e-02 -4.75244112e-02  5.51298968e-02  1.02863833e-02\n",
    "       2.45338436e-02  2.49001980e-02  1.16799213e-01  1.02445275e-01\n",
    "      -4.78395782e-02 -6.11801185e-02  4.37457450e-02 -3.72903571e-02\n",
    "      -4.15750444e-02 -9.71988365e-02 -1.97156565e-03  3.29912454e-02\n",
    "      -2.13280264e-02 -1.02769576e-01  3.61079946e-02 -2.64145657e-02\n",
    "       9.04297605e-02  3.54486369e-02 -2.91568544e-02  1.03625089e-01\n",
    "      -9.41040218e-02 -5.67003898e-02  3.77397910e-02 -4.47377227e-02\n",
    "       7.18963612e-03 -3.55827287e-02 -1.89791229e-02 -9.02059227e-02\n",
    "       1.89802162e-02 -5.60935773e-02  2.47590560e-02  1.01045854e-02\n",
    "       8.14194828e-02  6.65857196e-02  4.16135006e-02 -3.90387699e-02\n",
    "      -3.44039383e-03  3.06885336e-02  6.70636967e-02 -8.55752304e-02\n",
    "      -3.35187986e-02 -5.26400506e-02 -2.06645206e-02  3.60688232e-02\n",
    "       2.78380476e-02 -6.59141466e-02  3.60011384e-02  1.97536927e-02\n",
    "      -1.11820258e-01 -2.30906461e-03  1.15684690e-02  1.65773823e-03\n",
    "       1.37196228e-01  3.28815193e-03 -7.74928108e-02  2.85647027e-02\n",
    "      -6.02809153e-02  2.37368862e-03 -1.43561754e-02  7.00630173e-02\n",
    "      -5.43874614e-02 -8.03274885e-02  2.93609612e-02  1.94245726e-02\n",
    "      -2.18332093e-02 -1.22052524e-02 -7.11195022e-02  6.99475408e-02\n",
    "       1.22621581e-01 -5.81212305e-02  2.11172048e-02 -1.87510594e-08\n",
    "      -3.00814621e-02 -8.34934879e-03  5.99007457e-02  4.17810455e-02\n",
    "       9.00554005e-03  5.20310998e-02  8.32400378e-03  2.90580355e-02\n",
    "      -1.17555670e-01 -2.68037505e-02  1.09415941e-01  7.31720030e-02\n",
    "      -1.81163605e-02 -9.15972400e-04  1.19827932e-03 -4.20594066e-02\n",
    "       6.66432232e-02 -9.15124714e-02 -3.80621478e-02  1.68342795e-02\n",
    "      -5.94501756e-03  6.94455504e-02 -2.29016561e-02  5.60755245e-02\n",
    "       4.38208692e-02  2.39182962e-03  8.04815143e-02  6.93722144e-02\n",
    "      -5.77484295e-02 -1.89536773e-02  2.46403869e-02  9.27565023e-02\n",
    "      -2.64064949e-02 -4.52436656e-02  8.95928312e-03 -3.07379849e-02\n",
    "       1.30720526e-01  2.94232219e-02  5.37222512e-02 -2.19165273e-02\n",
    "      -2.31754389e-02 -4.73286919e-02 -2.98651028e-02  4.21220288e-02\n",
    "       5.72138652e-02 -3.41255691e-05  4.10156511e-02 -6.76238164e-02\n",
    "      -4.65301275e-02 -1.32226139e-01 -1.01639435e-01  4.56268806e-03\n",
    "      -9.53335315e-03  2.67508160e-02 -2.86023458e-03 -9.76563618e-03\n",
    "       5.95268910e-04 -3.51175256e-02  4.04534815e-03  6.09354451e-02\n",
    "       5.79103269e-03  1.06247805e-01  4.81397398e-02 -1.53533667e-02]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef949f44",
   "metadata": {},
   "source": [
    "## Semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601567d",
   "metadata": {},
   "source": [
    "### Using semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I need a desktop book reader for Mac\"\n",
    "\n",
    "# Generate embeddings\n",
    "query_embedding = embedder.encode([query])[0]\n",
    "\n",
    "# Compare embeddings\n",
    "hits = util.semantic_search(query_embedding, sentence_embeddings, top_k=2)\n",
    "\n",
    "# Print the top results\n",
    "for hit in hits[0]:\n",
    "    print(sentences[hit[\"corpus_id\"]], \"(Score: {:.4f})\".format(hit[\"score\"]))\n",
    "    \n",
    "\"\"\"\n",
    "Can anyone suggest a desktop book reader for Mac that works similar to Stanza on the iPhone? (Score: 0.8011)\n",
    "I'm looking for a good quality headset that doesn't cost too much. Any recommendations? (Score: 0.1437)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
